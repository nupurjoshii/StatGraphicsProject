# Data sources

Describe the data sources: who is responsible for collecting the data? How is it collected? If there were a choice of options, explain how you chose.

Provide some basic information about the dataset: types of variables, number of records, etc.

Describe any issues / problems with the data, either known or that you discover.

(suggested: approximately 1 page)


The data used for this project will be the historical price data and market capitalizations of Bitcoin, Ethereum and the S & P 500  and Dow Jones Indices over the past 5 years. The data for this is well documented and available via R libraries of tidyquant and coinmarketcapr. 

As these prices are very well documented online, any csv file could have been found and downloaded with similar results. However, obtaining them via pre set up R libraries makes the process much easier and very accessible. 

The data collected here is in the form of quantitative data over time in the form of a time series. We have daily prices, index, volume traded and dates as our different variables. We have decided to restrict our viewing to the last five years worth of data as that is when most of the crypto market boomed and grew. Looking at the last five years worth of data additionally makes it an interesting case study as a lot of world events happened and observing the changes in the different markets will be worth it.


TO INCLUDE OR NOT:
"""One avenue we attempted exploring but did not make much headway in was Twitter Sentiment Analysis for the crypto market. Essentially this process involved looking at twitter mentions of Bitcoin/BTC and Ethereum/ETH and trying to associate an increased number of twitter activity with the prices of these cryptocurrencies. This was difficult to explore as the daily tweet numbers we could manage to yield were restricted to 144,000 tweets / 24 hours; while recently the daily mentions exceed a million tweets. Thus, the process of gathering historical data for twitter mentions was mostly fruitless."""

Problems with Data:

Using the R library of tidyquant for the Dow Jones and S&P 500 worked very smoothly. However,the coinmarketcapr does not help us much as it only gives us current marketcap and price data, while we are interested in the historical view of such data. Hence, focusing on prices for the crypt

ANY PROBLEMS??


```{r setup, include=FALSE}
# this prevents package loading message from appearing in the rendered version of your problem set 
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
rm(list=ls())
library(devtools)
library(twitteR)
library(tm)
library(wordcloud)
library(ggplot2)
library(fpc)
library(base64enc)
library(igraph)
library("openssl")
library("httpuv")

#Part 1 - Twitter Data Download
#Application Details
options(httr_oauth_cache=T)
api_key='MZKRdmz4WnrJaNljztrnh0A6Q'
api_secret='L66Aez4pIjzfUWIbCywpktIYOeuvfP0sMIjxmW8dskje9Kw4O4'
access_token='31146251-phjrau5Yj6iUVCHGjATMYU9KyTi0WUdI7eqpwuODG'
access_token_secret='yzw5f8fcfaGx7iOwuzeWS1RM5QwguZUc8cLYQmZuWYA08'
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)

#Fetching tweets of input keywords 
web3_tweets= searchTwitter('Web3', n=1000,lang = 'en')
eth_tweets= searchTwitter('Ethereum', n=1000,lang = 'en')
btc_tweets= searchTwitter('Bitcoin', n=1000,lang = 'en')

web3_tweet_df=twListToDF(web3_tweets)
eth_tweet_df=twListToDF(eth_tweets)
btc_tweet_df=twListToDF(btc_tweets)

# Convert char date to correct date format
#web3
web3_tweet_df$created <- as.Date(web3_tweet_df$created, format= "%m/%d/%y")
web3_tweet_df$text <- as.character(web3_tweet_df$text)
str(web3_tweet_df)
#eth
eth_tweet_df$created <- as.Date(eth_tweet_df$created, format= "%m/%d/%y")
eth_tweet_df$text <- as.character(eth_tweet_df$text)
str(eth_tweet_df)
#btc
btc_tweet_df$created <- as.Date(btc_tweet_df$created, format= "%m/%d/%y")
btc_tweet_df$text <- as.character(btc_tweet_df$text)
str(btc_tweet_df)

```
#Check Non-modified, non-clean data word clouds to get a sense of what people are discussing about


```{r setup, include=FALSE}
# this prevents package loading message from appearing in the rendered version of your problem set 
knitr::opts_chunk$set(warning = FALSE, message = FALSE)



makeCorpus <- function(text){ #Function for making corpus and cleaning the tweets fetched
  twitterdf <- do.call("rbind", lapply(text, as.data.frame)) #store the fetched tweets as a data frame
  twitterdf$text <- sapply(twitterdf$text,function(row) iconv(row, "latin1", "ASCII", sub=""))#Removing emoticons from tweets
  twitterCorpus <- Corpus(VectorSource(twitterdf$text)) #Creating Corpus
  toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x)) #function to replace a pattern to white space using regex
  twitterCorpus <- tm_map(twitterCorpus, toSpace, "(RT|via)((?:\\b\\W*@\\w+)+)") #match rt or via
  twitterCorpus <- tm_map(twitterCorpus, toSpace, "@\\w+") #match @
  twitterCorpus <- tm_map(twitterCorpus, toSpace, "[ \t]{2,}") #match tabs
  twitterCorpus <- tm_map(twitterCorpus, toSpace, "[ |\n]{1,}") #match new lines
  twitterCorpus <- tm_map(twitterCorpus, toSpace, "^ ") #match white space at beginning
  twitterCorpus <- tm_map(twitterCorpus, toSpace, " $") #match white space at the end
  twitterCorpus <- tm_map(twitterCorpus, PlainTextDocument)
  twitterCorpus <- tm_map(twitterCorpus, removeNumbers)
  twitterCorpus <- tm_map(twitterCorpus, removePunctuation)
  twitterCorpus <- tm_map(twitterCorpus, toSpace, "http[[:alnum:]]*") #remove url from tweets
  twitterCorpus <- tm_map(twitterCorpus,removeWords,stopwords("en"))
  twitterCorpus <- tm_map(twitterCorpus, content_transformer(tolower))
  return(twitterCorpus)
}


#Wordcloud
makeWordcloud<-function (getText){ #plotting wordcloud
  twicorpus<-makeCorpus(getText)
  myTdm<-TermDocumentMatrix(twicorpus, control=list(wordLengths=c(4,Inf))) #Create TDM
  matrix<-as.matrix(myTdm)
  wordFreq <- sort(rowSums(matrix), decreasing=TRUE)#find frequency of words and sorting them in descending
  set.seed(375) 
  grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
  wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F,colors=grayLevels)
}




freqPlot<-function (getText){ #frequency plot of word count
  twicorpus<-makeCorpus(getText)
  myTdm<-TermDocumentMatrix(twicorpus, control=list(wordLengths=c(4,Inf)))
  matrix<-as.matrix(myTdm)
  termFrequency <- rowSums(matrix)
  termFrequency <- subset(termFrequency, termFrequency>=10)
  df <- data.frame(term=names(termFrequency), freq=termFrequency)
  ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + coord_flip()
}

#freqPlot(web3_tweets)
makeWordcloud(web3_tweets)
makeWordcloud(eth_tweets)
makeWordcloud(btc_tweets)
```

